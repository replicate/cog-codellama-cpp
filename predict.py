import os
import time
import subprocess
from cog import BasePredictor, Input, ConcatenateIterator
from llama_cpp import Llama

# This prompt formatting was copied from the original CodeLlama repo:
# https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/generation.py#L44

# These are components of the prompt that should not be changed by the users
B_INST, E_INST = "[INST]", "[/INST]"
B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
PROMPT_TEMPLATE = f"<s>{B_INST} {{instruction}} {E_INST}"
PROMPT_TEMPLATE_WITH_SYSTEM_PROMPT = f"<s>{B_INST} {B_SYS}{{system_prompt}}{E_SYS}{{instruction}} {E_INST}"

DEFAULT_SYSTEM_PROMPT = """"""

class Predictor(BasePredictor):
    def setup(self):
        # model.txt is generated by the Makefile
        with open("model.txt") as f:
            model = f.read().strip()
        self.is_instruct = "-instruct" in model
        model_path = f"/models/{model}"
        model_url = f"https://storage.googleapis.com/replicate-weights/llamacpp/{model}"
        start = time.time()
        if not os.path.exists(model_path):
            print("Downloading model weights....")
            subprocess.check_call(["pget", model_url, model_path])
            print("Downloading weights took: ", time.time() - start)
        self.llm = Llama(
            model_path, n_ctx=4096, n_gpu_layers=-1, main_gpu=0, n_threads=1
        )

    def predict(
        self,
        prompt: str = Input(description="Prompt"),
        system_prompt: str = Input(
            description="System prompt to send to CodeLlama. This is prepended to the prompt and helps guide system behavior.", 
            default=DEFAULT_SYSTEM_PROMPT,
        ),
        max_tokens: int = Input(
            description="Max number of tokens to return", default=500
        ),
        temperature: float = Input(description="Temperature", default=0.8),
        top_p: float = Input(description="Top P", default=0.95),
        top_k: int = Input(description="Top K", default=10),
        frequency_penalty: float = Input(
            description="Frequency penalty", ge=0.0, le=2.0, default=0.0
        ),
        presence_penalty: float = Input(
            description="Presence penalty", ge=0.0, le=2.0, default=0.0
        ),
        repeat_penalty: float = Input(
            description="Repetition penalty", ge=0.0, le=2.0, default=1.1
        ),
    ) -> ConcatenateIterator[str]:
        
        user_prompt = prompt.strip('\n').lstrip(B_INST).rstrip(E_INST).strip()
        prompt_templated = PROMPT_TEMPLATE.format(instruction=user_prompt)

        # If USE_SYSTEM_PROMPT is True, and the user has supplied some sort of system prompt, we add it to the prompt.
        if self.is_instruct and system_prompt != '':
            prompt_templated = PROMPT_TEMPLATE_WITH_SYSTEM_PROMPT.format(system_prompt=system_prompt, instruction=user_prompt)

        print("Prompt:\n" + prompt_templated)

        for tok in self.llm(
            prompt_templated,
            grammar=None,
            max_tokens=max_tokens,
            stream=True,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            repeat_penalty=repeat_penalty,
            mirostat_mode=0,
        ):
            yield tok["choices"][0]["text"]
