import os
import pathlib
import subprocess
import time

# model.txt is generated by the Makefile
with open("model.txt") as f:
    model = f.read().strip()
model_path = f"/models/{model}"
# model_url = f"https://storage.googleapis.com/replicate-weights/llamacpp/{model}"
if "34b-instruct" in model:
    # use accelerated storage for only the most frequently used
    model_url = f"replicate-weights.accel-object.lga1.coreweave.com/llamacpp/{model}"
else:
    model_url = f"replicate-weights.object.lga1.coreweave.com/llamacpp/{model}"

# don't download if we're running in docker (i.e. generating schema)
if (
    os.getenv("PGET")
    or not pathlib.Path("/.dockerenv").exists()
    and pathlib.Path(model_path).exists()
):
    pget_proc: subprocess.Popen | None = subprocess.Popen(
        ["/usr/bin/pget", model_url, model_path], close_fds=True
    )
    print("Downloading model weights...")
else:
    pget_proc = None

from cog import BasePredictor, Input, ConcatenateIterator
from llama_cpp import Llama


def wait_pget(file_name: str) -> None:
    for i in range(int(600 / 0.05)):
        if pathlib.Path(file_name).exists():
            break
        time.sleep(0.05)


class Predictor(BasePredictor):
    def setup(self) -> None:
        self.is_instruct = "-instruct" in model
        if pget_proc:
            if pget_proc.wait() != 0:
                print("Download failed, trying again")
                start = time.time()
                model_url = f"https://weights.replicate.delivery/llamacpp/{model}"
                subprocess.check_call(["pget", model_url, model_path])
                print("Downloading weights took: ", time.time() - start)
        else:
            print("error!! no pget proc")
            wait_pget(model_path)
        self.llm = Llama(
            model_path, n_ctx=4096, n_gpu_layers=-1, main_gpu=0, n_threads=1
        )

    def predict(
        self,
        prompt: str = Input(description="Prompt"),
        max_tokens: int = Input(
            description="Max number of tokens to return", default=500
        ),
        temperature: float = Input(description="Temperature", default=0.8),
        top_p: float = Input(description="Top P", default=0.95),
        top_k: int = Input(description="Top K", default=10),
        frequency_penalty: float = Input(
            description="Frequency penalty", ge=0.0, le=2.0, default=0.0
        ),
        presence_penalty: float = Input(
            description="Presence penalty", ge=0.0, le=2.0, default=0.0
        ),
        repeat_penalty: float = Input(
            description="Repetition penalty", ge=0.0, le=2.0, default=1.1
        ),
    ) -> ConcatenateIterator[str]:
        if self.is_instruct:
            prompt = f"<s>[INST] {prompt} [/INST]"
        print("Prompt:\n" + prompt)

        for tok in self.llm(
            prompt,
            grammar=None,
            max_tokens=max_tokens,
            stream=True,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            repeat_penalty=repeat_penalty,
            mirostat_mode=0,
        ):
            yield tok["choices"][0]["text"]
