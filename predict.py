import os
import time
import subprocess
from cog import BasePredictor, Input, ConcatenateIterator
from llama_cpp import Llama


class Predictor(BasePredictor):
    def setup(self):
        # model.txt is generated by the Makefile
        with open("model.txt") as f:
            model = f.read().strip()
        self.is_instruct = "-instruct" in model
        model_path = f"/models/{model}"
        model_url = f"https://storage.googleapis.com/replicate-weights/llamacpp/{model}"
        start = time.time()
        if not os.path.exists(model_path):
            print("Downloading model weights....")
            subprocess.check_call(["pget", model_url, model_path])
            print("Downloading weights took: ", time.time() - start)
        self.llm = Llama(
            model_path, n_ctx=4096, n_gpu_layers=-1, main_gpu=0, n_threads=1
        )

    def predict(
        self,
        prompt: str = Input(description="Prompt"),
        max_tokens: int = Input(
            description="Max number of tokens to return", default=500
        ),
        temperature: float = Input(description="Temperature", default=0.8),
        top_p: float = Input(description="Top P", default=0.95),
        top_k: int = Input(description="Top K", default=10),
        frequency_penalty: float = Input(
            description="Frequency penalty", ge=0.0, le=2.0, default=0.0
        ),
        presence_penalty: float = Input(
            description="Presence penalty", ge=0.0, le=2.0, default=0.0
        ),
        repeat_penalty: float = Input(
            description="Repetition penalty", ge=0.0, le=2.0, default=1.1
        ),
    ) -> ConcatenateIterator[str]:
        if self.is_instruct:
            prompt = f"<s>[INST] {prompt} [/INST]"
        print("Prompt:\n" + prompt)

        for tok in self.llm(
            prompt,
            grammar=None,
            max_tokens=max_tokens,
            stream=True,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            repeat_penalty=repeat_penalty,
            mirostat_mode=0,
        ):
            yield tok["choices"][0]["text"]
